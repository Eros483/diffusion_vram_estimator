{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38b2bdec22d0be4a",
   "metadata": {},
   "source": [
    "# GPU-vRAM Usage Estimation for Diffusion Models\n",
    "## Objective\n",
    "Derive an analytical equation to estimate peak vRAM usage during inference for the `stable-diffusion-v1-5/stable-diffusion-v1-5` for arbitrary input image sizes.\n",
    "\n",
    "## Background\n",
    "vRAM consumption during diffusion model inference differs significantly from model size on disk. Peak memory depends on:\n",
    " - Model weights (fixed)\n",
    " - Intermediate activations (vary with image dimensions and prompt length)\n",
    " - Framework overhead (CUDA kernels, workspace buffers)\n",
    " - Attention mechanism memory scaling (O(N²) with sequence length)\n",
    "\n",
    "Where:\n",
    " - `H`, `W` = input image height and width\n",
    " - `prompt_length` = tokenized prompt length\n",
    " - Identify any additional factors affecting vRAM\n",
    "\n",
    "## Requirements\n",
    " - Analyze the architecture: Understand UNet, VAE, CLIP text encoder, and how tensors flow through the pipeline\n",
    " - Account for precision: Assume `FP16` (2 bytes/parameter)\n",
    " - Model fully on GPU: Ignore pipeline.enable_model_cpu_offload() in your equation\n",
    " - Peak, not average: Find the stage with maximum memory allocation\n",
    " - Document assumptions: Clearly state what you include/exclude (e.g., gradient storage, optimizer states)\n",
    "\n",
    "## Deliverables\n",
    " - Equation with explanation of each term\n",
    " - Derivation notes showing how you arrived at each component\n",
    " - Validation (optional but encouraged): Compare equation predictions against actual nvidia-smi measurements using the provided test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57fb1fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# cuda torch verification\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6e92a5dab124454",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arnab/miniconda3/envs/deepLure/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00,  8.84it/s]\n",
      "100%|██████████| 40/40 [00:04<00:00,  8.51it/s]\n",
      "100%|██████████| 40/40 [04:16<00:00,  6.40s/it]\n",
      "100%|██████████| 40/40 [00:10<00:00,  3.81it/s]\n",
      "100%|██████████| 40/40 [01:28<00:00,  2.22s/it]\n"
     ]
    }
   ],
   "source": [
    "# pip install torch torchvision diffusers['torch'] transformers accelerate\n",
    "\n",
    "from diffusers import AutoPipelineForImage2Image\n",
    "from diffusers.utils import make_image_grid, load_image\n",
    "\n",
    "pipeline = AutoPipelineForImage2Image.from_pretrained(\n",
    "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
    ")\n",
    "pipeline = pipeline.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Uncomment this if you have limited GPU vRAM (although, this assignment can be done without any GPU use!)\n",
    "pipeline.enable_model_cpu_offload()\n",
    "\n",
    "# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed\n",
    "pipeline.enable_xformers_memory_efficient_attention()\n",
    "\n",
    "# prepare image\n",
    "img_src = [{\n",
    "    \"url\": \"./data/balloon--low-res.jpeg\",\n",
    "    \"prompt\": \"aerial view, colorful hot air balloon, lush green forest canopy, springtime, warm climate, vibrant foliage, soft sunlight, gentle shadow, white birds flying alongside, harmony, freedom, bright natural colors, serene atmosphere, highly detailed, realistic, photorealistic, cinematic lighting\"\n",
    "}, {\n",
    "    'url': \"./data/bench--high-res.jpg\",\n",
    "    'prompt': \"photorealistic, high resolution, realistic lighting, natural shadows, detailed textures, lush green grass, wooden bench with grain detail, expansive valley, agricultural fields, blue-toned mountains, fluffy cumulus clouds, wispy cirrus clouds, bright blue sky, clear sunny day, soft sunlight, tranquil atmosphere, cinematic realism\"\n",
    "}, {\n",
    "    'url': \"./data/groceries--low-res.jpg\",\n",
    "    'prompt': \"cartoon style, bold outlines, simplified shapes, vibrant colors, playful atmosphere, exaggerated proportions, stylized SUV trunk, whimsical paper grocery bags, fresh produce with bright highlights, baguette with cartoon detail, cheerful parking area, greenery with simplified textures, sunny day, lighthearted mood, 2D illustration, animated landscape aesthetic\"\n",
    "}, {\n",
    "    'url': \"./data/truck--high-res.jpg\",\n",
    "    'prompt': \"Michelangelo style, Renaissance painting, classical composition, rich earthy tones, detailed brushwork, divine atmosphere, expressive lighting, monumental presence, artistic grandeur, fresco-inspired texture, high contrast shadows, timeless aesthetic\"\n",
    "    #quirk noticed, both images very dissimiliar, neither does a truck fit the description, objectively performs worse than every other prompt/image pair.\n",
    "}]\n",
    "\n",
    "results = list()\n",
    "\n",
    "# This for loop is meant to demonstrate that the models' vRAM usage depends\n",
    "# on Image-size and prompt length (among other factors). You may observe the\n",
    "# vRAM usage while the model is running by executing the following command\n",
    "# in a separate terminal and monitoring the changes in vRAM usage:\n",
    "#    ```shell\n",
    "#    watch -n 1.0 nvidia-smi\n",
    "#    ```\n",
    "#\n",
    "# You may modify this for loop according to your needs.\n",
    "for _src in img_src:\n",
    "    init_image = load_image(_src.get('url'))\n",
    "    prompt = _src.get('prompt')\n",
    "\n",
    "    # pass prompt and image to pipeline\n",
    "    image = pipeline(prompt, image=init_image, guidance_scale=5.0).images[0]\n",
    "    results.append(make_image_grid([init_image, image], rows=1, cols=2))\n",
    "\n",
    "results[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e53d0962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "\n",
    "for i, res in enumerate(results):\n",
    "    res.save(f\"./output/result_{i}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bba4ca34",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder=pipeline.text_encoder\n",
    "unet=pipeline.unet\n",
    "vae=pipeline.vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1336c2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total parameters of each model\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "clip_params=count_params(text_encoder)\n",
    "unet_params=count_params(unet)\n",
    "vae_params=count_params(vae)\n",
    "\n",
    "clip_weights_mb=(clip_params*2)/(1024*1024)\n",
    "unet_weights_mb=(unet_params*2)/(1024*1024)\n",
    "vae_weights_mb=(vae_params*2)/(1024*1024)\n",
    "\n",
    "total_weights_mb=clip_weights_mb+unet_weights_mb+vae_weights_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "643de3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# referencing vae configuration values below\n",
    "vae_config=vae.config\n",
    "latent_channels=vae_config.latent_channels #verified as 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b4a3e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_factor=vae.config.scaling_factor\n",
    "vae_scale_factor=1/scaling_factor #used as 8 in papers, but here choosing from model actual config\n",
    "\n",
    "vae_block_channels=vae.config.block_out_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9f69c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128, 256, 512, 512]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_block_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41d637d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# referencing config of clip below\n",
    "clip_config=text_encoder.config\n",
    "clip_hidden_dim=clip_config.hidden_size\n",
    "clip_num_layers=clip_config.num_hidden_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a001300",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_config=unet.config\n",
    "unet_in_channels=unet_config.in_channels #4 for latent space\n",
    "unet_block_out_channels=unet_config.block_out_channels\n",
    "unet_down_block_types=unet_config.down_block_types\n",
    "unet_up_block_types=unet_config.up_block_types\n",
    "unet_attention_head_dim=unet_config.attention_head_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a957cc0fae58e3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def f(h: int, w: int, prompt_length: int, guidance_scale: float=7.5, **kwargs):\n",
    "#     \"\"\"\n",
    "#     :param h: height of input image in pixels\n",
    "#     :param w: width of input image in pixels\n",
    "#     :param prompt_length: length of input prompt in number tokens generated after tokenizing the input-prompt.\n",
    "#     :param kwargs: any additional factors needed for this computation (this is for your use)\n",
    "\n",
    "#     Args:\n",
    "#         guidance_scale (float, optional): The guidance scale as defined in Classifier-Free Diffusion Guidance. Defaults to 7.5.\n",
    "#     \"\"\"\n",
    "#     latent_h=h//vae_scale_factor\n",
    "#     latent_w=w//vae_scale_factor\n",
    "\n",
    "#     cfg_multiplier=2 if guidance_scale>1.0 else 1\n",
    "#     text_embeddings_mb=(cfg_multiplier*prompt_length*clip_hidden_dim*2)/(1024*1024)\n",
    "\n",
    "#     # clip attention o(prompt length squared) for 12 layers\n",
    "#     clip_attention_mb=(12*prompt_length*prompt_length*2)/(1024*1024)\n",
    "#     clip_activations_mb=text_embeddings_mb+clip_attention_mb\n",
    "\n",
    "#     # coarse approximation of unet feature maps at 4 levels\n",
    "#     scales=[\n",
    "#         (latent_h, latent_w, unet_block_out_channels[0]),\n",
    "#         (latent_h//2, latent_w//2, unet_block_out_channels[1]),\n",
    "#         (latent_h//4, latent_w//4, unet_block_out_channels[2]),\n",
    "#         (latent_h//8, latent_w//8, unet_block_out_channels[3])\n",
    "#     ]\n",
    "\n",
    "#     unet_features_mb=0\n",
    "#     for sh, sw, sc in scales:\n",
    "#         f_m=cfg_multiplier*sh*sw*sc\n",
    "#         unet_features_mb+=(f_m*2)/(1024*1024)\n",
    "\n",
    "#     cross_attention_mb=0\n",
    "#     for sh, sw, sc in scales:\n",
    "#         spatial_tokens=sh*sw\n",
    "\n",
    "#         num_head=sc//unet_attention_head_dim\n",
    "#         attention_map=cfg_multiplier*spatial_tokens*prompt_length*num_head\n",
    "#         cross_attention_mb+=(attention_map*2)/(1024*1024)\n",
    "\n",
    "#     self_attention_mb=0\n",
    "#     for sh, sw, sc in scales[:2]: # only last two levels have self-attention layers\n",
    "#         spatial_tokens=sh*sw\n",
    "\n",
    "#         num_head=sc//unet_attention_head_dim\n",
    "#         attention_map=cfg_multiplier*spatial_tokens*spatial_tokens*num_head\n",
    "#         self_attention_mb+=(attention_map*2)/(1024*1024)\n",
    "\n",
    "#     unet_activations_mb=unet_features_mb+cross_attention_mb+self_attention_mb\n",
    "\n",
    "#     vae_scales=[\n",
    "#     (latent_h, latent_w, vae_block_channels[0]),\n",
    "#     (latent_h*2, latent_w*2, vae_block_channels[1]),\n",
    "#     (latent_h*4, latent_w*4, vae_block_channels[2]),\n",
    "#     (latent_h*8, latent_w*8, vae_block_channels[3]),\n",
    "#     (h, w, 3)\n",
    "#     ]\n",
    "\n",
    "#     vae_activations_mb=0\n",
    "#     for sh, sw, sc in vae_scales:\n",
    "#         f_m=2*sh*sw*sc\n",
    "#         vae_activations_mb+=(f_m*2)/(1024*1024)\n",
    "    \n",
    "#     #not including in pytorch and cuda overhead as unreliable and no direct calculation from pytorch\n",
    "\n",
    "#     peak_vram_mb=(\n",
    "#         total_weights_mb+\n",
    "#         clip_activations_mb+\n",
    "#         unet_activations_mb\n",
    "#         # (vae_activations_mb) vae decode step happens after denoising completes and not during it, so cannot contribute to peak memory so only activates after Unet has been cleared\n",
    "#     )\n",
    "\n",
    "#     return peak_vram_mb\n",
    "\n",
    "\n",
    "# Massively incorrect results even if accounting for optimizations and xformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d7b1d91a2374d6",
   "metadata": {},
   "source": [
    "## Your Task\n",
    "Derive a formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12acf889",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc3bc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def f(h: int, w: int, prompt_length: int, guidance_scale: float=5.0, **kwargs):\n",
    "#     \"\"\"\n",
    "#     :param h: height of input image in pixels\n",
    "#     :param w: width of input image in pixels\n",
    "#     :param prompt_length: length of input prompt in number tokens generated after tokenizing the input-prompt.\n",
    "#     :param kwargs: any additional factors needed for this computation (this is for your use)\n",
    "\n",
    "#     Args:\n",
    "#         guidance_scale (float, optional): The guidance scale as defined in Classifier-Free Diffusion Guidance. Defaults to 5.0.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     #Model works well for small images, but fails for larger images, thus pointing to growth w.r.t size and unrealistic constant vram demand w.r.t size.\n",
    "\n",
    "#     latent_h=h//vae_scale_factor\n",
    "#     latent_w=w//vae_scale_factor\n",
    "\n",
    "#     cfg_multiplier=2 if guidance_scale>1.0 else 1\n",
    "#     text_embeddings_mb=(cfg_multiplier*prompt_length*clip_hidden_dim*2)/(1024*1024)\n",
    "\n",
    "#     #removing CLIP attention, happens once at the start, doesnt exist simultaneously with unet which is likely the bottleneck\n",
    "#     #adding UNET features only at the peak resolution level   ----  REDACTED change --- after test run and review of results\n",
    "\n",
    "#     scales=[\n",
    "#         (latent_h, latent_w, unet_block_out_channels[0]),\n",
    "#         (latent_h//2, latent_w//2, unet_block_out_channels[1]),\n",
    "#         (latent_h//4, latent_w//4, unet_block_out_channels[2]),\n",
    "#         (latent_h//8, latent_w//8, unet_block_out_channels[3])\n",
    "#     ]\n",
    "#     unet_features_mb=0\n",
    "#     for sh, sw, sc in scales: # only first two levels should likely be contributing to peak\n",
    "#         f_m=cfg_multiplier*sh*sw*sc\n",
    "#         unet_features_mb+=(f_m*2)/(1024*1024)\n",
    "\n",
    "#     peak_h=scales[0][0]\n",
    "#     peak_w=scales[0][1]\n",
    "#     peak_c=scales[0][2]\n",
    "\n",
    "#     #removing cross attention and self attention at all scales, accounting for xformers change o(nxm) instead of o(n^2) for cross attention, and o(n) self attention instead of o(n2)\n",
    "#     # adding in number of heads for cross attention calculations\n",
    "#     spatial_tokens=peak_h*peak_w\n",
    "#     num_heads=peak_c//unet_attention_head_dim\n",
    "#     cross_attention_mb=(cfg_multiplier*spatial_tokens*prompt_length*2*num_heads)*(2)/(1024*1024)\n",
    "#     self_attention_mb=(cfg_multiplier*spatial_tokens*2)/(1024*1024)\n",
    "\n",
    "#     #accounting for residual connections in the UNET\n",
    "#     residual_activations_mb=unet_features_mb*0.3\n",
    "\n",
    "#     unet_activations_mb=unet_features_mb+cross_attention_mb+self_attention_mb+residual_activations_mb\n",
    "\n",
    "#     #remove VAE entirely, but retain latent encoding by VAE\n",
    "#     vae_latent_mb=(2*latent_h*latent_w*latent_channels)/(1024*1024)\n",
    "\n",
    "#     #last-ditch effort, accounting for overhead that scales non linearly with activation size, perhaps explains failure to grow for larger images, from cuda context, caching memory allocator, etc, arbitarily choosing value of 0.3\n",
    "#     overhead_mb=(unet_activations_mb+text_embeddings_mb)*0.3\n",
    "\n",
    "#     peak_vram_mb=(\n",
    "#         total_weights_mb+\n",
    "#         text_embeddings_mb+\n",
    "#         unet_activations_mb+\n",
    "#         vae_latent_mb+\n",
    "#         overhead_mb\n",
    "#     )\n",
    "\n",
    "#     return peak_vram_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2078b247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(h: int, w: int, prompt_length: int, guidance_scale: float=5.0, **kwargs):\n",
    "    \"\"\"\n",
    "    :param h: height of input image in pixels\n",
    "    :param w: width of input image in pixels\n",
    "    :param prompt_length: length of input prompt in number tokens generated after tokenizing the input-prompt.\n",
    "    :param kwargs: any additional factors needed for this computation (this is for your use)\n",
    "\n",
    "    Args:\n",
    "        guidance_scale (float, optional): The guidance scale as defined in Classifier-Free Diffusion Guidance. Defaults to 5.0.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Model works well for small images, but fails for larger images, thus pointing to growth w.r.t size and unrealistic constant vram demand w.r.t size.\n",
    "\n",
    "    latent_h=h//vae_scale_factor\n",
    "    latent_w=w//vae_scale_factor\n",
    "\n",
    "    cfg_multiplier=2 if guidance_scale>1.0 else 1\n",
    "    text_embeddings_mb=(cfg_multiplier*prompt_length*clip_hidden_dim*2)/(1024*1024)\n",
    "\n",
    "    #removing CLIP attention, happens once at the start, doesnt exist simultaneously with unet which is likely the bottleneck\n",
    "    #adding UNET features only at the peak resolution level   ----  REDACTED change --- after test run and review of results\n",
    "\n",
    "    scales=[\n",
    "        (latent_h, latent_w, unet_block_out_channels[0]),\n",
    "        (latent_h//2, latent_w//2, unet_block_out_channels[1]),\n",
    "        (latent_h//4, latent_w//4, unet_block_out_channels[2]),\n",
    "        (latent_h//8, latent_w//8, unet_block_out_channels[3])\n",
    "    ]\n",
    "    unet_features_mb=0\n",
    "    for sh, sw, sc in scales: # only first two levels should likely be contributing to peak\n",
    "        f_m=cfg_multiplier*sh*sw*sc\n",
    "        unet_features_mb+=(f_m*2*3)/(1024*1024) # ACCOUNTING FOR RESIDUAL CONNECTIONS HERE INPUT, OUTPUT, SKIP\n",
    "\n",
    "    peak_h=scales[0][0]\n",
    "    peak_w=scales[0][1]\n",
    "    peak_c=scales[0][2]\n",
    "\n",
    "    # ---- REDACTED ---- removing cross attention and self attention at all scales, accounting for xformers change o(nxm) instead of o(n^2) for cross attention, and o(n) self attention instead of o(n2)\n",
    "    # adding in number of heads for cross attention calculations\n",
    "    spatial_tokens=peak_h*peak_w\n",
    "    num_heads=peak_c//unet_attention_head_dim\n",
    "\n",
    "\n",
    "    # ACCOUNT FOR QKV CACHE and MATRICES\n",
    "    total_attention_mb = 0\n",
    "    for sh, sw, sc in scales[:2]:  # First 2 scales typically have attention blocks\n",
    "        spatial_tokens = sh * sw\n",
    "        \n",
    "        # Cross-attention: needs Q (from spatial), K, V (from text), and output\n",
    "        # Q: [cfg_mult, spatial_tokens, channels]\n",
    "        # K, V: [cfg_mult, prompt_length, channels] \n",
    "        # Output: [cfg_mult, spatial_tokens, channels]\n",
    "        cross_attention_mb = (\n",
    "            cfg_multiplier * spatial_tokens * sc +           # Q projection\n",
    "            cfg_multiplier * prompt_length * sc * 2 +        # K, V projections\n",
    "            cfg_multiplier * spatial_tokens * sc             # Attention output\n",
    "        ) * 2 / (1024 * 1024)\n",
    "        \n",
    "        # Self-attention: Q, K, V all from spatial tokens\n",
    "        # Even with xformers/flash attention, we need Q, K, V matrices\n",
    "        self_attention_mb = (\n",
    "            cfg_multiplier * 3 * spatial_tokens * sc +       # Q, K, V projections\n",
    "            cfg_multiplier * spatial_tokens * sc             # Attention output\n",
    "        ) * 2 / (1024 * 1024)\n",
    "        \n",
    "        total_attention_mb += cross_attention_mb + self_attention_mb\n",
    "\n",
    "\n",
    "    unet_activations_mb=unet_features_mb+total_attention_mb\n",
    "\n",
    "    #remove VAE entirely, but retain latent encoding by VAE\n",
    "    vae_latent_mb=(2*latent_h*latent_w*latent_channels)/(1024*1024)\n",
    "\n",
    "    #last-ditch effort, accounting for overhead that scales non linearly with activation size, perhaps explains failure to grow for larger images, from cuda context, caching memory allocator, etc, arbitarily choosing value of 0.3\n",
    "    overhead_mb=(unet_activations_mb+text_embeddings_mb)*0.3\n",
    "\n",
    "    peak_vram_mb=(\n",
    "        total_weights_mb+\n",
    "        text_embeddings_mb+\n",
    "        unet_activations_mb+\n",
    "        vae_latent_mb+\n",
    "        overhead_mb\n",
    "    )\n",
    "\n",
    "    return peak_vram_mb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd160f4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a12f04e",
   "metadata": {},
   "source": [
    "## Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "16648c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:04<00:00,  9.18it/s]\n",
      "100%|██████████| 40/40 [04:21<00:00,  6.53s/it]\n",
      "100%|██████████| 40/40 [00:10<00:00,  3.79it/s]\n",
      "100%|██████████| 40/40 [01:28<00:00,  2.22s/it]\n"
     ]
    }
   ],
   "source": [
    "results=[]\n",
    "\n",
    "for idx in range(len(img_src)):\n",
    "    # clear cache before measurement\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    test_case=img_src[idx]\n",
    "    init_image = load_image(test_case.get('url'))\n",
    "    prompt=test_case.get('prompt')\n",
    "\n",
    "    h, w=init_image.size[1], init_image.size[0]\n",
    "    tokenized_prompt=pipeline.tokenizer(prompt, return_tensors=\"pt\")\n",
    "    prompt_length=tokenized_prompt.input_ids.shape[1]\n",
    "\n",
    "    image=pipeline(prompt, image=init_image, guidance_scale=7.5).images[0]\n",
    "\n",
    "    actual_peak_mb=torch.cuda.max_memory_allocated()/ (1024*1024)\n",
    "    predicted_peak_mb=f(h, w, prompt_length)\n",
    "    delta=actual_peak_mb - predicted_peak_mb\n",
    "\n",
    "    results.append({\n",
    "    \"idx\": idx,\n",
    "    \"h\": h,\n",
    "    \"w\": w,\n",
    "    \"prompt_length\": prompt_length,\n",
    "    \"predicted_peak_mb\": predicted_peak_mb,\n",
    "    \"actual_peak_mb\": actual_peak_mb,\n",
    "    \"delta_mb\": delta\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e3ea44e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'idx': 0,\n",
       "  'h': 380,\n",
       "  'w': 396,\n",
       "  'prompt_length': 55,\n",
       "  'predicted_peak_mb': 2147.5687465667725,\n",
       "  'actual_peak_mb': 2490.1533203125,\n",
       "  'delta_mb': 342.58457374572754},\n",
       " {'idx': 1,\n",
       "  'h': 2048,\n",
       "  'w': 2048,\n",
       "  'prompt_length': 64,\n",
       "  'predicted_peak_mb': 5215.882328414917,\n",
       "  'actual_peak_mb': 6906.06591796875,\n",
       "  'delta_mb': 1690.1835895538334},\n",
       " {'idx': 2,\n",
       "  'h': 534,\n",
       "  'w': 800,\n",
       "  'prompt_length': 66,\n",
       "  'predicted_peak_mb': 2354.8342205047607,\n",
       "  'actual_peak_mb': 2619.16748046875,\n",
       "  'delta_mb': 264.33325996398935},\n",
       " {'idx': 3,\n",
       "  'h': 1200,\n",
       "  'w': 1800,\n",
       "  'prompt_length': 43,\n",
       "  'predicted_peak_mb': 3664.287310409546,\n",
       "  'actual_peak_mb': 5771.58837890625,\n",
       "  'delta_mb': 2107.301068496704}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b159d7ea",
   "metadata": {},
   "source": [
    "### **model consistently fails to grow enough for larger predictions, near margin of error for smaller images** \n",
    "- temporarily resolved by unbacked CUDA+PyTorch overhead assumption (resolved to some extent, issue still exists)\n",
    "\n",
    "### **Issues**\n",
    "- Rectangular images might need some form of padding for better performance\n",
    "- Unsure of perhaps existence of inverse relationship between prompt token length vs vram consumption\n",
    "- above 2 points, focused on idx 3, as it breaks the previously thought issue of predictions growing faultier as image gets larger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28818909",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66967b01fd33f683",
   "metadata": {},
   "source": [
    "## Tips\n",
    "- Although no GPU is needed to accomplish this task (analyze code/architecture)\n",
    "- Use PyTorch documentation and model architecture inspection\n",
    "\n",
    "# Evaluation Criteria\n",
    "- Correctness: Formula accounts for major memory consumers\n",
    "- Completeness: All image-dependent and prompt-dependent factors identified\n",
    "- Rigor: Derivation shows understanding of PyTorch memory model and diffusion architecture\n",
    "- Clarity: Equation is readable and well-documented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefb1615",
   "metadata": {},
   "source": [
    "---\n",
    "### Research insights into building equation\n",
    "Diffusion model consists of\n",
    "    - CLIP encoder\n",
    "    - UNet\n",
    "    - VAE Decoder\n",
    "\n",
    "These have the highest number of parameters thus affect the vram occupancy the most\n",
    "\n",
    "1. CLIP encoder\n",
    "- Convert text into features\n",
    "- So basically context embeddings, like used in RNNs and LSTMs\n",
    "- standard transformer architecture with mhsa (multihead self attention)\n",
    "- 123M parameters\n",
    "\n",
    "2. UNet\n",
    "- Starts via noise\n",
    "- denoises image via attention with text embeddings\n",
    "- (not the standard conv UNet as used for convolution)\n",
    "- 859M parameters\n",
    "- likely consumes the most amount of vram\n",
    "\n",
    "3. VAE decoder\n",
    "- intractible density function functioning on latent space as produced by Unet\n",
    "- guides it back into pixel space\n",
    "- 83.65M parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209ae36c",
   "metadata": {},
   "source": [
    "---\n",
    "Rough work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "358858a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StableDiffusionImg2ImgPipeline {\n",
      "  \"_class_name\": \"StableDiffusionImg2ImgPipeline\",\n",
      "  \"_diffusers_version\": \"0.35.2\",\n",
      "  \"_name_or_path\": \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n",
      "  \"feature_extractor\": [\n",
      "    \"transformers\",\n",
      "    \"CLIPImageProcessor\"\n",
      "  ],\n",
      "  \"image_encoder\": [\n",
      "    null,\n",
      "    null\n",
      "  ],\n",
      "  \"requires_safety_checker\": true,\n",
      "  \"safety_checker\": [\n",
      "    \"stable_diffusion\",\n",
      "    \"StableDiffusionSafetyChecker\"\n",
      "  ],\n",
      "  \"scheduler\": [\n",
      "    \"diffusers\",\n",
      "    \"PNDMScheduler\"\n",
      "  ],\n",
      "  \"text_encoder\": [\n",
      "    \"transformers\",\n",
      "    \"CLIPTextModel\"\n",
      "  ],\n",
      "  \"tokenizer\": [\n",
      "    \"transformers\",\n",
      "    \"CLIPTokenizer\"\n",
      "  ],\n",
      "  \"unet\": [\n",
      "    \"diffusers\",\n",
      "    \"UNet2DConditionModel\"\n",
      "  ],\n",
      "  \"vae\": [\n",
      "    \"diffusers\",\n",
      "    \"AutoencoderKL\"\n",
      "  ]\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "202a1902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenDict([('in_channels', 3), ('out_channels', 3), ('down_block_types', ['DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D']), ('up_block_types', ['UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D']), ('block_out_channels', [128, 256, 512, 512]), ('layers_per_block', 2), ('act_fn', 'silu'), ('latent_channels', 4), ('norm_num_groups', 32), ('sample_size', 512), ('scaling_factor', 0.18215), ('shift_factor', None), ('latents_mean', None), ('latents_std', None), ('force_upcast', True), ('use_quant_conv', True), ('use_post_quant_conv', True), ('mid_block_add_attention', True), ('_use_default_values', ['latents_mean', 'mid_block_add_attention', 'scaling_factor', 'shift_factor', 'use_quant_conv', 'latents_std', 'force_upcast', 'use_post_quant_conv']), ('_class_name', 'AutoencoderKL'), ('_diffusers_version', '0.6.0'), ('_name_or_path', '/home/arnab/.cache/huggingface/hub/models--stable-diffusion-v1-5--stable-diffusion-v1-5/snapshots/451f4fe16113bff5a5d2269ed5ad43b0592e9a14/vae')])\n"
     ]
    }
   ],
   "source": [
    "print(vae.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8db57e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIPTextConfig {\n",
      "  \"architectures\": [\n",
      "    \"CLIPTextModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"dropout\": 0.0,\n",
      "  \"dtype\": \"float16\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"quick_gelu\",\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 77,\n",
      "  \"model_type\": \"clip_text_model\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"projection_dim\": 768,\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"vocab_size\": 49408\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text_encoder.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8bcae58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenDict([('sample_size', 64), ('in_channels', 4), ('out_channels', 4), ('center_input_sample', False), ('flip_sin_to_cos', True), ('freq_shift', 0), ('down_block_types', ['CrossAttnDownBlock2D', 'CrossAttnDownBlock2D', 'CrossAttnDownBlock2D', 'DownBlock2D']), ('mid_block_type', 'UNetMidBlock2DCrossAttn'), ('up_block_types', ['UpBlock2D', 'CrossAttnUpBlock2D', 'CrossAttnUpBlock2D', 'CrossAttnUpBlock2D']), ('only_cross_attention', False), ('block_out_channels', [320, 640, 1280, 1280]), ('layers_per_block', 2), ('downsample_padding', 1), ('mid_block_scale_factor', 1), ('dropout', 0.0), ('act_fn', 'silu'), ('norm_num_groups', 32), ('norm_eps', 1e-05), ('cross_attention_dim', 768), ('transformer_layers_per_block', 1), ('reverse_transformer_layers_per_block', None), ('encoder_hid_dim', None), ('encoder_hid_dim_type', None), ('attention_head_dim', 8), ('num_attention_heads', None), ('dual_cross_attention', False), ('use_linear_projection', False), ('class_embed_type', None), ('addition_embed_type', None), ('addition_time_embed_dim', None), ('num_class_embeds', None), ('upcast_attention', False), ('resnet_time_scale_shift', 'default'), ('resnet_skip_time_act', False), ('resnet_out_scale_factor', 1.0), ('time_embedding_type', 'positional'), ('time_embedding_dim', None), ('time_embedding_act_fn', None), ('timestep_post_act', None), ('time_cond_proj_dim', None), ('conv_in_kernel', 3), ('conv_out_kernel', 3), ('projection_class_embeddings_input_dim', None), ('attention_type', 'default'), ('class_embeddings_concat', False), ('mid_block_only_cross_attention', None), ('cross_attention_norm', None), ('addition_embed_type_num_heads', 64), ('_use_default_values', ['conv_out_kernel', 'time_embedding_type', 'upcast_attention', 'resnet_out_scale_factor', 'only_cross_attention', 'reverse_transformer_layers_per_block', 'resnet_time_scale_shift', 'class_embed_type', 'class_embeddings_concat', 'addition_embed_type_num_heads', 'addition_time_embed_dim', 'projection_class_embeddings_input_dim', 'encoder_hid_dim_type', 'transformer_layers_per_block', 'dual_cross_attention', 'encoder_hid_dim', 'time_cond_proj_dim', 'num_attention_heads', 'time_embedding_act_fn', 'resnet_skip_time_act', 'attention_type', 'addition_embed_type', 'timestep_post_act', 'mid_block_only_cross_attention', 'time_embedding_dim', 'mid_block_type', 'num_class_embeds', 'use_linear_projection', 'conv_in_kernel', 'dropout', 'cross_attention_norm']), ('_class_name', 'UNet2DConditionModel'), ('_diffusers_version', '0.6.0'), ('_name_or_path', '/home/arnab/.cache/huggingface/hub/models--stable-diffusion-v1-5--stable-diffusion-v1-5/snapshots/451f4fe16113bff5a5d2269ed5ad43b0592e9a14/unet')])\n"
     ]
    }
   ],
   "source": [
    "print(unet.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2cce974b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet_config.layers_per_block"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepLure",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
